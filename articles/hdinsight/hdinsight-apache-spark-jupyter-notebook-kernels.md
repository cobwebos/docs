---
title: "将不同的内核用于 Azure HDInsight Spark 群集上的 Jupyter 笔记本 | Microsoft 文档"
description: "了解 HDInsight Linux 上的 Spark 群集所包含的 Jupyter 笔记本使用的 PySpark、PySpark3 和 Spark 内核。"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/10/2017
ms.author: nitinme
ms.translationtype: Human Translation
ms.sourcegitcommit: 97fa1d1d4dd81b055d5d3a10b6d812eaa9b86214
ms.openlocfilehash: 096fbc1d30e8c9df0c9008525e0fac3fd6e449cf
ms.contentlocale: zh-cn
ms.lasthandoff: 05/11/2017


---
# <a name="jupyter-notebooks-kernels-with-apache-spark-clusters-in-hdinsight"></a>HDInsight 中 Apache Spark 群集所包含的 Jupyter 笔记本内核 

HDInsight Spark 群集提供可在 Jupyter 笔记本中用于测试 Spark 应用程序的内核。 内核是可以运行和解释代码的程序。 这两个内核是：

- **PySpark** - 适用于以 Python2 编写的应用程序
- **PySpark3** - 适用于以 Python3 编写的应用程序
- **Spark** - 适用于以 Scala 编写的应用程序

在本文中，你将学习如何使用这些内核以及使用它们的优势。

## <a name="prerequisites"></a>先决条件

* HDInsight 中的 Apache Spark 群集。 有关说明，请参阅[在 Azure HDInsight 中创建 Apache Spark 群集](hdinsight-apache-spark-jupyter-spark-sql.md)。

## <a name="create-a-jupyter-notebook"></a>创建 Jupyter 笔记本

1. 从 [Azure 门户网站](https://portal.azure.com/)打开群集。  有关说明，请参阅[列出和显示群集](hdinsight-administer-use-portal-linux.md#list-and-show-clusters)。 将在新的门户边栏选项卡中打开群集。

2. 在“快速链接”部分中，单击“群集仪表板”打开“群集仪表板”边栏选项卡。  如果没有看到“快速链接”，请从边栏选项卡上的左侧菜单中单击“概述”。

    ![群集仪表板](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-azure-portal-cluster-dashboards.png "群集仪表板") 

3. 单击“Jupyter 笔记本”。 出现提示时，请输入群集的管理员凭据。
   
   > [!NOTE]
   > 也可以在浏览器中打开以下 URL 来访问群集的 Jupyter 笔记本。 将 **CLUSTERNAME** 替换为群集的名称：
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. 单击“新建”，然后单击“Pyspark”、“PySpark3”或“Spark”创建笔记本。 使用适用于 Scala 应用程序的 Spark 内核，适用于 Python2 应用程序的 PySpark 内核，以及适用于 Python3 应用程序的 PySpark3 内核。
   
    ![创建 Jupyter 笔记本](./media/hdinsight-apache-spark-jupyter-notebook-kernels/jupyter-kernels.png "创建 Jupyter 笔记本") 

4. 此时将打开使用所选内核的笔记本。

## <a name="benefits-of-using-these-kernels"></a>使用这些内核的好处

下面是使用新内核的几项好处。

- **预设上下文**。 使用 **PySpark**、**PySpark3** 或 **Spark** 内核时，无需首先显式设置 Spark 或 Hive 上下文，即可开始使用应用程序。 这些上下文默认已提供。 这些上下文包括：
   
   * **sc** - 表示 Spark 上下文
   * **sqlContext** - 表示 Hive 上下文

    因此，不需要运行如下语句来设置上下文：

          sc = SparkContext('yarn-client')
          sqlContext = HiveContext(sc)

    可以直接在应用程序中使用预设上下文。

- **单元 magic**。 PySpark 内核提供一些预定义的“magic”，这是可以结合 `%%` 调用的特殊命令（例如 `%%MAGIC` <args>）。 magic 命令必须是代码单元中的第一个字，并且允许多行内容。 magic 一字应该是单元中的第一个字。 在 magic 前面添加任何内容（即使是注释）会导致错误。     有关 magic 的详细信息，请参阅[此处](http://ipython.readthedocs.org/en/stable/interactive/magics.html)。
   
    下表列出可通过内核提供的不同 magic。

   | Magic | 示例 | 说明 |
   | --- | --- | --- |
   | help |`%%help` |生成所有可用 magic 的表，其中包含示例和说明 |
   | info |`%%info` |输出当前 Livy 终结点的会话信息 |
   | 配置 |`%%configure -f`<br>`{"executorMemory": "1000M"`,<br>`"executorCores": 4`} |配置用于创建会话的参数。 如果已创建会话，则必须指定 force 标志 (-f)，确保删除再重新创建该会话。 有关有效参数的列表，请查看 [Livy 的 POST /sessions 请求正文](https://github.com/cloudera/livy#request-body)。 参数必须以 JSON 字符串传入，并且必须位于 magic 后面的下一行，如示例列中所示。 |
   | sql |`%%sql -o <variable name>`<br> `SHOW TABLES` |针对 sqlContext 执行 Hive 查询。 如果传递了 `-o` 参数，则查询的结果将以 [Pandas](http://pandas.pydata.org/) 数据帧的形式保存在 %%local Python 上下文中。 |
   | local |`%%local`<br>`a=1` |后续行中的所有代码在本地执行。 不管使用哪种内核，代码都必须是有效的 Python2 代码。 因此，即使在创建笔记本时选择了“PySpark3”或“Spark”，但如果在单元中使用 `%%local` magic，该单元也只能包含有效的 Python2 代码。 |
   | 日志 |`%%logs` |输出当前 Livy 会话的日志。 |
   | 删除 |`%%delete -f -s <session number>` |删除当前 Livy 终结点的特定会话。 请注意，无法删除针对内核本身启动的会话。 |
   | cleanup |`%%cleanup -f` |删除当前 Livy 终结点的所有会话，包括此笔记本的会话。 force 标志 -f 是必需的。 |

   > [!NOTE]
   > 除了 PySpark 内核添加的 magic 以外，还可以使用[内置的 IPython magic](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics)（包括 `%%sh`）。 可以使用 `%%sh` magic 在群集头节点上运行脚本和代码块。
   >
   >
2. **自动可视化**。 **Pyspark** 内核自动将 Hive 和 SQL 查询的输出可视化。 可以选择多种不同类型的视觉效果，包括表、饼图、折线图、分区图和条形图。

## <a name="parameters-supported-with-the-sql-magic"></a>%%sql magic 支持的参数
`%%sql` magic 支持不同的参数，可以使用这些参数控制运行查询时收到的输出类型。 下表列出了输出。

| 参数 | 示例 | 说明 |
| --- | --- | --- |
| -o |`-o <VARIABLE NAME>` |使用此参数将查询结果以 [Pandas](http://pandas.pydata.org/) 数据帧的形式保存在 %%local Python 上下文中。 数据帧变量的名称是指定的变量名称。 |
| -q |`-q` |使用此参数可关闭单元可视化。 如果不想自动将单元内容可视化，而只想将它作为数据帧捕获，可以使用 `-q -o <VARIABLE>`。 如果想要关闭可视化而不捕获结果（例如，运行诸如 `CREATE TABLE` 语句的 SQL 查询），请使用不带 `-o` 参数的 `-q`。 |
| -m |`-m <METHOD>` |其中，**METHOD** 是 **take** 或 **sample**（默认为 **take**）。 如果方法为 **take**，内核将从 MAXROWS 指定的结果数据集顶部选择元素（如此表中稍后所述）。 如果方法为 **sample**，内核将根据 `-r` 参数进行数据集的元素随机采样，如此表中稍后所述。 |
| -r |`-r <FRACTION>` |此处的 **FRACTION** 是介于 0.0 与 1.0 之间的浮点数。 如果 SQL 查询的采样方法为 `sample`，则内核将针对结果集元素的指定分数随机采样。 例如，如果使用参数 `-m sample -r 0.01` 运行 SQL 查询，则会随机采样 1% 的结果行。 |
| -n |`-n <MAXROWS>` |**MAXROWS** 是整数值。 内核将输出行的数目限制为 **MAXROWS**。 如果 **MAXROWS** 是负数（例如 **-1**），结果集中的行数不受限制。 |

**示例：**

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

上述语句执行以下操作：

* 从 **hivesampletable** 中选择所有记录。
* 由于使用了 -q，因此将关闭自动可视化。
* 由于使用了 `-m sample -r 0.1 -n 500`，因此将从 hivesampletable 的行中随机采样 10%，并将结果集的大小限制为 500 行。
* 最后，由于使用了 `-o query2`，因此也会将输出保存到名为 **query2** 的数据帧中。

## <a name="considerations-while-using-the-new-kernels"></a>使用新内核时的注意事项

不管使用哪种内核，使笔记本一直保持运行都会消耗群集资源。  使用这些内核时，由于上下文是预设的，仅退出笔记本并不会终止上下文，因此会继续占用群集资源。 合理的做法是在使用完笔记本后，使用笔记本“文件”菜单中的“关闭并停止”选项来终止上下文，然后退出笔记本。     

## <a name="show-me-some-examples"></a>举例说明

打开 Jupyter 笔记本时，可以在根级别看到两个文件夹。

* **PySpark** 文件夹包含使用新 **Python** 内核的示例笔记本。
* **Scala** 文件夹包含使用新 **Spark** 内核的示例笔记本。

可从 **PySpark** 或 **Spark** 文件夹打开 **00 - [READ ME FIRST] Spark Magic Kernel Features** 笔记本，了解各种可用的 magic。 也可以使用这两个文件夹下面提供的其他笔记本示例，了解如何在 HDInsight Spark 群集上实现不同的 Jupyter 笔记本使用方案。

## <a name="where-are-the-notebooks-stored"></a>笔记本存储在何处？

Jupyter 笔记本保存在与 **/HdiNotebooks** 文件夹下面的群集关联的存储帐户中。  可以从存储帐户访问在 Jupyter 内部创建的笔记本、文本文件和文件夹。  例如，如果使用 Jupyter 创建文件夹 **myfolder** 和笔记本 **myfolder/mynotebook.ipynb**，可在存储帐户中通过 `/HdiNotebooks/myfolder/mynotebook.ipynb` 访问该笔记本。  反之亦然，如果直接将笔记本上载到 `/HdiNotebooks/mynotebook1.ipynb` 中的存储帐户，则可以从 Jupyter 查看该笔记本。  即使删除了群集，笔记本也仍会保留在存储帐户中。

将笔记本保存到存储帐户的方式与 HDFS 兼容。 因此，如果通过 SSH 访问群集，可按以下代码片段中所示使用文件管理命令：

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


笔记本还会保存到头节点 `/var/lib/jupyter`，以防止访问群集的存储帐户时出现问题。

## <a name="supported-browser"></a>支持的浏览器

Google Chrome 仅支持针对 HDInsight Spark 群集运行的 Jupyter 笔记本。

## <a name="feedback"></a>反馈
新内核正处于发展阶段，一段时间后将变得成熟。 这可能也意味着，API 可随着这些内核的成熟而改变。 如果在使用这些新内核时有任何反馈，我们将不胜感激。 这对于内核最终版本的定调会很有帮助。 可以在本文末尾的**意见**部分下面填写意见/反馈。

## <a name="seealso"></a>另请参阅
* [概述：Azure HDInsight 上的 Apache Spark](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a>方案
* [Spark 和 BI：使用 HDInsight 中的 Spark 和 BI 工具执行交互式数据分析](hdinsight-apache-spark-use-bi-tools.md)
* [Spark 和机器学习：使用 HDInsight 中的 Spark 对使用 HVAC 数据生成温度进行分析](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [Spark 流式处理：使用 HDInsight 中的 Spark 生成实时流式处理应用程序](hdinsight-apache-spark-eventhub-streaming.md)
* [使用 HDInsight 中的 Spark 分析网站日志](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a>创建和运行应用程序
* [使用 Scala 创建独立的应用程序](hdinsight-apache-spark-create-standalone-application.md)
* [使用 Livy 在 Spark 群集中远程运行作业](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a>工具和扩展
* [使用适用于 IntelliJ IDEA 的 HDInsight 工具插件创建和提交 Spark Scala 应用程序](hdinsight-apache-spark-intellij-tool-plugin.md)
* [Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely（使用 IntelliJ IDEA 的 HDInsight 工具插件远程调试 Spark 应用程序）](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](hdinsight-apache-spark-use-zeppelin-notebook.md)
* [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a>管理资源
* [管理 Azure HDInsight 中 Apache Spark 群集的资源](hdinsight-apache-spark-resource-manager.md)
* [Track and debug jobs running on an Apache Spark cluster in HDInsight（跟踪和调试 HDInsight 中的 Apache Spark 群集上运行的作业）](hdinsight-apache-spark-job-debugging.md)

